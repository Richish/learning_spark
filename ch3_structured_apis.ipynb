{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earlier implementation:\n",
    "an RDD has a compute function that produces an Iterator[T] for the\n",
    "data that will be stored in the RDD.\n",
    "\n",
    "The compute function (or computation) is opaque to Spark. That is, Spark does\n",
    "not know what you are doing in the compute function. Whether you are performing\n",
    "a join, filter, select, or aggregation, Spark only sees it as a lambda expression. Another\n",
    "problem is that the Iterator[T] data type is also opaque for Python RDDs; Spark\n",
    "only knows that it’s a generic object in Python.\n",
    "Furthermore, because it’s unable to inspect the computation or expression in the\n",
    "function, Spark has no way to optimize the expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New implementation\n",
    "Spark 2.x introduced a few key schemes for structuring Spark. \n",
    "One is to express computations\n",
    "by using common patterns found in data analysis. These patterns are\n",
    "expressed as high-level operations such as filtering, selecting, counting, aggregating,\n",
    "averaging, and grouping. This provides added clarity and simplicity.\n",
    "\n",
    "This specificity is further narrowed through the use of a set of common operators in a\n",
    "DSL. Through a set of operations in DSL, available as APIs in Spark’s supported languages\n",
    "(Java, Python, Spark, R, and SQL), these operators let you tell Spark what you\n",
    "wish to compute with your data, and as a result, it can construct an efficient query\n",
    "plan for execution.\n",
    "\n",
    "And the final scheme of order and structure is to allow you to arrange your data in a\n",
    "tabular format, like a SQL table or spreadsheet, with supported structured data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to work\n",
    "we want to aggregate all the ages for each name, group by\n",
    "name, and then average the ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## low level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brooke', 22.5)\n",
      "('Denny', 31.0)\n",
      "('Jules', 30.0)\n",
      "('TD', 35.0)\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "sc = SparkContext(\"local\", \"SparkOldImplementation\")\n",
    "data_rdd = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)])\n",
    "\n",
    "ages_rdd = data_rdd.map(lambda x: (x[0], (x[1],1))).reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])).map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "for age_info in ages_rdd.collect():\n",
    "    print(age_info)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using structured apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkStructuredApis\").getOrCreate()\n",
    "\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "ages_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "ages_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
